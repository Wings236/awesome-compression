# 第7章 低秩分解

![低秩分解示意图](images/low_rank.png)

&emsp;&emsp;本章我们将逐步学习低秩分解的理论基础与实践应用。7.1节首先对低秩分解进行了总体介绍，7.2节深入阐释了矩阵的“秩”这一核心概念及其分解思想。7.3节详细讲解了多种主流的分解方法，包括作为理论基石的奇异值分解（SVD）、面向高维张量的Tucker分解和CP分解，以及强调可解释性的非负矩阵分解（NMF）。随后，7.4节系统性地回答了为何要进行低秩分解，分析了其在模型压缩、计算加速、特征提取和去噪等方面的价值。7.5节探讨了在分解中至关重要的“秩”的选择策略。7.6节指出了在深度学习模型中可以应用低秩分解的具体对象。最后，7.7节将通过实践案例来巩固所学知识。

## 低秩分解介绍 

&emsp;&emsp;在数据科学和机器学习的广阔领域中，我们处理的数据形式常常是矩阵。无论是代表用户对电影评分的矩阵，还是深度神经网络中的权重矩阵，这些高维数据结构中往往蕴含着深刻的模式和信息。然而，这些矩阵的规模通常非常庞大，带来了巨大的计算和存储挑战。低秩分解（ Low-Rank Decomposition 或 Low-Rank Factorization ）技术应运而生，旨在揭示并利用这些大型矩阵中固有的低维结构。

&emsp;&emsp;低秩分解的核心思想源于一个关键的观察：深度神经网络中的权重矩阵往往是**过参数化**的，其中存在着大量的**冗余**。这意味着矩阵中的许多行或列向量之间存在着线性相关性，其所代表的变换并非真正需要那么多的参数来表达。低秩分解正是利用了权重矩阵的这种内在冗余性，将一个大的权重矩阵近似分解为两个或多个更小的“瘦”矩阵的乘积。

![低秩分解示意图](images/dizhifenjie.png)

&emsp;&emsp;这种分解不仅是一种数据压缩技术，更是一种强大的数据分析手段。通过将数据投影到一个更低维度的“潜在因子”空间，我们不仅能够显著减少计算复杂度和存储需求，还能在一定程度上滤除数据中的噪声，并揭示出数据背后最本质的特征和结构。例如，在推荐系统中，这些潜在因子可能代表用户的某种偏好和物品的某种属性；在基因数据分析中，它们可能对应着特定的地理或族群信息等。

&emsp;&emsp;低秩分解涵盖了一系列具体的方法，其中最著名和最基础的是**奇异值分解（Singular Value Decomposition, SVD）**。此外，还包括**非负矩阵分解（Non-negative Matrix Factorization, NMF）**、**Tucker分解**和**CP分解**等。这些技术虽然在具体算法和应用场景上有所不同，但它们的根本目标是一致的：寻找数据最简洁、最核心的表达方式。在本章中，我们将深入探讨低秩分解的基本原理、核心思想及其在模型压缩等领域的关键作用。

## 何为低秩分解 (What is Low-Rank Decomposition?)

&emsp;&emsp;为了深刻理解低秩分解，我们首先需要明确两个核心概念：什么是矩阵的“秩”，以及分解的“核心思想”是什么。

### 矩阵的秩

&emsp;&emsp;矩阵的**秩（Rank）**是线性代数中的一个基本概念，它衡量了矩阵中所包含的“内在维度”或“信息量”。一个矩阵的秩，被严格定义为其**列向量（或行向量）中线性无关向量的最大数量**。换句话说，秩表示了能够构建出矩阵所有行或所有列的“基向量”的最少数量。

&emsp;&emsp;一个向量组是**线性无关**的，意味着其中任何一个向量都不能被其他向量的线性组合所表示。相反，如果一个向量可以被其他向量表示，那么它就是**线性相关**的，代表着信息上的冗余。

&emsp;&emsp;让我们用一个简单的例子来说明。考虑以下矩阵 $A$：

$$
A = \begin{bmatrix} 1 & 2 & 3 \\ 2 & 4 & 6 \\ 3 & 6 & 9 \end{bmatrix}
$$

我们可以观察到，第二行向量 $[2, 4, 6]$ 恰好是第一行向量 $[1, 2, 3]$ 的2倍。同样，第三行 $[3, 6, 9]$ 是第一行的3倍。这意味着，这个矩阵的所有行向量都可以由一个基础向量 $[1, 2, 3]$ 乘以不同的系数得到。因此，这个矩阵的行向量中只有一个是线性无关的。同样地，第二列是第一列的2倍，第三列是第一列的3倍。所以，矩阵 $A$ 的秩为1。

一个矩阵的秩 $r$ 满足 $0 \le r \le \min(m, n)$，其中 $m$ 和 $n$ 分别是矩阵的行数和列数。
* 当 $r = \min(m, n)$ 时，我们称该矩阵为**满秩（Full-Rank）**矩阵，表示其行或列向量是完全线性无关的，信息没有冗余。
* 当 $r < \min(m, n)$ 时，我们称该矩阵为**低秩（Low-Rank）**或**秩亏缺（Rank-Deficient）**矩阵。这意味着矩阵中存在冗余信息，其行或列向量可以由一个更小的向量集合线性表示。

&emsp;&emsp;在神经网络的权重矩阵或推荐系统的评分矩阵等实际应用中，许多大型矩阵虽然理论上可能是满秩的，但它们的“有效秩”或“近似秩”通常很低。这意味着矩阵的许多奇异值非常接近于零，其绝大部分信息和能量都集中在少数几个最大的奇异值所对应的方向上。这正是低秩分解能够发挥作用的根本原因。

### 低秩分解的核心思想

&emsp;&emsp;低秩分解的核心思想是利用矩阵的秩亏缺特性，将一个原始的大型矩阵 $A \in \mathbb{R}^{m \times n}$ 近似地分解为两个（或多个）尺寸更小的矩阵的乘积。最常见的形式是将其分解为两个矩阵 $B$ 和 $C$ 的乘积：

$$
A \approx B C^T
$$
其中，$B \in \mathbb{R}^{m \times k}$，$C \in \mathbb{R}^{n \times k}$（因此 $C^T \in \mathbb{R}^{k \times n}$）。这里的 $k$ 是我们选择的一个远小于 $m$ 和 $n$ 的正整数，它代表了近似矩阵的秩。

**这个分解过程的意义在于：**

1. **参数数量的显著减少**：
   * 存储原始矩阵 $A$ 需要的参数数量为 $m \times n$。
   * 存储分解后的矩阵 $B$ 和 $C^T$ 需要的参数数量为 $(m \times k) + (k \times n) = k \times (m+n)$。
   * 当 $k \ll \min(m, n)$ 时，参数量会得到极大的压缩。例如，一个 $1000 \times 1000$ 的矩阵有 $10^6$ 个参数。如果我们可以用 $k=50$ 的秩来近似它，那么分解后的参数数量仅为 $50 \times (1000+1000) = 100,000$，压缩了10倍。
2. **计算效率的提升**：
   * 对于一个输入向量 $x \in \mathbb{R}^n$，计算矩阵-向量乘积 $y = Ax$ 需要大约 $m \times n$ 次乘法和加法操作。
   * 使用分解后的矩阵，计算过程变为两步：首先计算 $z = C^T x$（需要 $k \times n$ 次操作），然后计算 $y = Bz$（需要 $m \times k$ 次操作）。总操作数约为 $k \times (m+n)$。
   * 这同样实现了计算复杂度的显著降低，从而加速了模型的推理过程。

![线性空间分解过程示意图](images/yinshefenjie.png)

&emsp;&emsp;这个分解过程可以被看作是寻找一个“潜在因子空间”（latent factor space），其维度为 $k$。矩阵 $B$ 的每一行可以被看作是原始矩阵 $A$ 的每一行在这个 $k$ 维潜在空间中的坐标表示。而矩阵 $C$ 的每一行（即 $C^T$ 的每一列）则可以被看作是构成这个潜在空间的 $k$ 个基向量。原始矩阵中的每个元素 $A_{ij}$ 就可以通过第 $i$ 行在潜在空间的坐标（$B$的第$i$行）与第 $j$ 列对应的基向量（$C$的第$j$行）的点积来近似重构。

## 低秩分解常见方法

### 奇异值分解(SVD)

奇异值分解（SVD）是低秩分解理论的基石，它不仅是一种具体的分解方法，更提供了低秩近似的**理论最优解**。

**1. 什么是SVD？**

任何一个秩为 $r$ 的矩阵 $W \in \mathbb{R}^{m \times n}$ 都可以被分解为三个矩阵的乘积：

$$
W = U \Sigma V^T
$$

其中：

* $U \in \mathbb{R}^{m \times m}$ 是一个**正交矩阵**（$U^T U = I$），其列向量 $u_i$ 被称为**左奇异向量**。
* $\Sigma \in \mathbb{R}^{m \times n}$ 是一个**对角矩阵**，其对角线上的元素 $\sigma_1, \sigma_2, \dots, \sigma_r$ 被称为**奇异值**。它们是非负的，并且按照从大到小的顺序排列：$\sigma_1 \ge \sigma_2 \ge \dots \ge \sigma_r > 0$。其余的对角线元素均为0。
* $V^T \in \mathbb{R}^{n \times n}$ 也是一个**正交矩阵**（$V^T V = I$），其行向量（即$V$的列向量 $v_i$）被称为**右奇异向量**。

这个分解也可以写成求和的形式：

$$
W = \sum_{i=1}^{r} \sigma_i u_i v_i^T
$$

这个表达式非常直观：它表明任何矩阵都可以表示为一系列秩为1的矩阵（$u_i v_i^T$）的加权和，权重就是对应的奇异值 $\sigma_i$。

**2. 如何通过SVD实现低秩近似？**

&emsp;&emsp;SVD的强大之处在于，奇异值 $\sigma_i$ 的大小直接反映了其对应的分量 $u_i v_i^T$ 对原始矩阵 $W$ 的“贡献度”。最大的奇异值 $\sigma_1$ 对应的分量捕捉了矩阵最主要的变化方向和信息，而较小的奇异值通常对应于数据中的细节或噪声。

因此，要获得一个秩为 $k$（$k<r$）的最佳近似矩阵 $W_k$，我们只需保留SVD分解中前 $k$ 个最大的奇异值及其对应的左右奇异向量，并舍弃其余的部分。具体操作如下：

1. 将对角矩阵 $\Sigma$ 中的第 $k+1$ 到第 $r$ 个奇异值置为零，得到一个新的对角矩阵 $\Sigma_k$。
2. 只取 $U$ 的前 $k$ 列，得到 $U_k \in \mathbb{R}^{m \times k}$。
3. 只取 $V^T$ 的前 $k$ 行，得到 $V_k^T \in \mathbb{R}^{k \times n}$。

这样，我们得到的秩为 $k$ 的近似矩阵 $W_k$ 为：

$$
W_k = U_k \Sigma_k V_k^T = \sum_{i=1}^{k} \sigma_i u_i v_i^T
$$

![SVD分解示意图](images/svd.png)


**3. 为什么SVD是最佳低秩逼近？—— Eckart-Young-Mirsky定理**

&emsp;&emsp;SVD之所以在低秩近似中如此核心，是因为它能找到在给定秩的情况下，与原始矩阵“最接近”的近似矩阵。这里的“最接近”通常用**弗罗贝尼乌斯范数（Frobenius Norm）**来度量。

&emsp;&emsp;一个矩阵 $A$ 的弗罗贝尼乌斯范数定义为：

$$
\|A\|_F = \sqrt{\sum_{i=1}^{m} \sum_{j=1}^{n} |a_{ij}|^2}
$$

&emsp;&emsp;它相当于将矩阵所有元素的平方和开根号，可以看作是欧几里得范数在矩阵上的推广。

**Eckart-Young-Mirsky定理（1936年）**指出：在所有秩不超过 $k$ 的矩阵 $B$ 中，由SVD截断得到的矩阵 $W_k$ 是使得弗罗贝尼乌斯范数 $\|W - B\|_F$ 最小化的矩阵。即：
$$
\min_{\text{rank}(B) \le k} \|W - B\|_F = \|W - W_k\|_F
$$

**为什么此结论成立？**

&emsp;&emsp;我们可以通过SVD的性质来理解这一点。首先，由于 $U$ 和 $V$ 都是正交矩阵，弗罗贝尼乌斯范数在正交变换下保持不变。原始矩阵 $W$ 的范数可以表示为奇异值的函数：

$$
\|W\|_F^2 = \left\|\sum_{i=1}^{r} \sigma_i u_i v_i^T\right\|_F^2 = \sum_{i=1}^{r} \sigma_i^2
$$

&emsp;&emsp;当我们计算 $W$ 和它的秩 - $k$ 近似 $W_k$ 之间的差时：

$$
W - W_k = \left(\sum_{i=1}^{r} \sigma_i u_i v_i^T\right) - \left(\sum_{i=1}^{k} \sigma_i u_i v_i^T\right) = \sum_{i=k+1}^{r} \sigma_i u_i v_i^T
$$

&emsp;&emsp;这个差的弗罗贝尼乌斯范数的平方就是被舍弃掉的奇异值的平方和：

$$
\|W - W_k\|_F^2 = \left\|\sum_{i=k+1}^{r} \sigma_i u_i v_i^T\right\|_F^2 = \sum_{i=k+1}^{r} \sigma_i^2
$$

&emsp;&emsp;该定理的证明表明，任何其他的秩 - $k$ 近似矩阵 $B$ 与 $W$ 的差的范数平方，都会大于或等于这个值。由于我们在SVD截断中保留了最大的 $k$ 个奇异值，也就是舍弃了最小的 $r-k$ 个奇异值，因此我们最小化了重构误差。SVD为我们提供了一个系统性的、最优的方法来“压缩”矩阵，同时最大程度地保留原始矩阵的信息。

&emsp;&emsp;在实际操作中，为了得到 $W \approx U'V'^T$ 的形式，我们可以让 $U' = U_k$ 和 $V'^T = \Sigma_k V_k^T$，或者 $U' = U_k \sqrt{\Sigma_k}$ 和 $V'^T = \sqrt{\Sigma_k} V_k^T$。这样就将三矩阵分解转化为了两矩阵分解，方便后续的计算。

**4. SVD的优点与局限**

&emsp;&emsp;优点：**普遍性** ：与某些分解方法（例如 LU 分解或 Cholesky 分解）要求矩阵具有特定属性（例如方阵或正定）不同，SVD 适用于任何形状的矩阵。**鲁棒性** ：SVD 在数值上是稳定的，这意味着它在计算中是可靠的，特别是在涉及噪声数据或近似解的场景中。**数据压缩和降维** ：SVD 的一个关键特性是它能够识别数据中最重要的部分，使其成为数据压缩和降维的理想工具。**理论优雅** ：SVD 具有深刻的理论含义，涉及许多重要的数学概念，如特征值、特征向量和欧几里得空间中的正交性。

&emsp;&emsp;局限性：**数据规模** ：对于极其庞大的数据集，SVD 的计算可能非常耗时且占用大量资源。尽管存在优化算法和近似方法，但大规模数据 SVD 计算仍然是一个挑战。**数据稀疏性** ：对于高度稀疏的数据集（例如，包含大量零元素的矩阵），标准 SVD 可能不是最有效的方法。需要一些特殊的技术来有效地处理数据的稀疏性。**对噪声的敏感性** ：SVD 对噪声敏感，尤其是在数据降维或特征提取等应用中。高水平的噪声会影响奇异值的计算，从而导致不准确的结果。**解释挑战** ：虽然 SVD 可以揭示数据的底层结构，但解释这些结构的含义并不总是那么简单。在某些应用中，例如社交网络分析或生物信息学，解释 SVD 的结果可能需要额外的领域知识。

**5. 数值稳定性和计算效率**

&emsp;&emsp;在计算 SVD 时，数值稳定性和计算效率是两个关键考虑因素：**数值稳定性** ：稳定性在计算特征值和特征向量时至关重要，因为微小的数值误差可能会导致结果出现显著偏差。通常采用以稳定性著称的雅可比方法或 QR 算法等算法来实现这一目的。**计算效率** ：大型矩阵的奇异值分解 (SVD) 计算可能极其耗时。为了提高效率，人们会利用各种优化技术（例如随机算法或分块算法）来加快计算过程。

&emsp;&emsp;在实际应用中，通常使用专门的数值线性代数库来执行 SVD。这些库针对稳定性和效率进行了优化。MATLAB、NumPy 和 SciPy 等工具提供了高效稳定的 SVD 实现。使用这些库可以确保结果的准确性，并在处理实际问题时保持较高的计算效率。

### Tucker分解

&emsp;&emsp;在7.4.1节中，我们探讨了奇异值分解（SVD）如何为二维数据（矩阵）提供最佳的低秩近似。然而，现实世界中的许多数据本质上是多维的，例如一张彩色图片（`高度` x `宽度` x `颜色通道`）、一段视频（`高度` x `宽度` x `颜色通道` x `时间`），或是推荐系统中的用户-商品-情境数据。将这些高维数据强行“压平”成矩阵（例如，将图片的所有像素拉成一个长向量）会破坏其固有的多维结构，丢失维度间的交互信息。

&emsp;&emsp;为了解决这个问题，我们需要将低秩分解的思想从二维矩阵推广到高维数据结构——**张量（Tensor）**。**Tucker分解**正是这一推广中最重要的方法之一，可以被看作是SVD在高维空间的一种自然延伸。

**1. 什么是Tucker分解？**

&emsp;&emsp;Tucker分解（Tucker Decomposition），也称为高阶SVD，是一种将高维张量分解为一个**核心张量**与一系列**因子矩阵**乘积的方法。

&emsp;&emsp;对于一个N阶张量 $\mathcal{X} \in \mathbb{R}^{I_1 \times I_2 \times \dots \times I_N}$，其Tucker分解可以表示为：
$$
\mathcal{X} \approx \mathcal{G} \times_1 U^{(1)} \times_2 U^{(2)} \times_3 \dots \times_N U^{(N)}
$$

&emsp;&emsp;这个公式可以进一步写成元素求和的形式：

$$
x_{i_1 i_2 \dots i_N} \approx \sum_{r_1=1}^{R_1} \sum_{r_2=1}^{R_2} \dots \sum_{r_N=1}^{R_N} g_{r_1 r_2 \dots r_N} u_{i_1 r_1}^{(1)} u_{i_2 r_2}^{(2)} \dots u_{i_N r_N}^{(N)}
$$

&emsp;&emsp;让我们来解析这个核心公式的每个组成部分：

- $\mathcal{X}$ (原始张量)：这是我们要分解的高维数据，例如一个 $512 \times 512 \times 3$ 的彩色图像张量。
- $\mathcal{G}$ (核心张量)：这是一个尺寸为 $R_1 \times R_2 \times \dots \times R_N$ 的小张量，其中 $R_n \le I_n$。核心张量 $\mathcal{G}$ 捕捉了不同因子矩阵之间交互的“权重”或“强度”。它本身是稠密的，描述了从原始数据中提取出的潜在特征（主成分）之间的相互关系。如果说因子矩阵是各个维度的“基”，那么核心张量就是这些“基”如何组合起来重构原始数据的“系数”。
- $U^{(n)}$ (因子矩阵)：对于张量的第 $n$ 个维度（或称为**模(mode)**），都有一个对应的因子矩阵 $U^{(n)} \in \mathbb{R}^{I_n \times R_n}$。这个矩阵的列向量构成了第 $n$ 维的一个低维正交基。你可以将其理解为该维度上的**主成分**。通常，我们要求这些因子矩阵是**列正交**的，即 $(U^{(n)})^T U^{(n)} = I$。
- $\times_n$ (n-模乘积)：这是一个张量与矩阵的特殊乘法运算。张量 $\mathcal{A}$ 与矩阵 $B$ 的n-模乘积，记作 $\mathcal{A} \times_n B$，是在张量 $\mathcal{A}$ 的第 $n$ 个维度上，将其每个**模-n纤维（mode-n fiber）**与矩阵 $B$ 进行相乘。

![Tucker分解示意图](images/Tucker.png)

**2. 理解Tucker分解的内在逻辑：模态展开与SVD**

&emsp;&emsp;Tucker分解的精髓在于，它对张量的每一个维度（模态）都独立地进行降维。这个过程可以通过一个叫做**模态展开（Unfolding / Matricization）**的操作来理解。

&emsp;&emsp;**模态-n展开**是将一个N阶张量 $\mathcal{X}$ 重新排列成一个矩阵 $X_{(n)}$ 的过程。在这个矩阵中，张量的第 $n$ 维的索引作为行索引，而所有其他 $N-1$ 个维度的索引被“拉平”后作为列索引。因此，矩阵 $X_{(n)}$ 的尺寸为 $I_n \times (I_1 \dots I_{n-1} I_{n+1} \dots I_N)$。

&emsp;&emsp;有了模态展开的概念后，Tucker分解的内在逻辑变得清晰： **因子矩阵 $U^{(n)}$ 的列向量，正是模态-n展开矩阵 $X_{(n)}$ 的前 $R_n$ 个左奇异向量。**

&emsp;&emsp;换句话说，为了找到第 $n$ 个维度上最重要的$R_n$个基向量，Tucker分解将张量沿该维度“摊平”成一个大矩阵，然后对这个矩阵进行SVD，取其最重要的左奇异向量。这个过程对每一个维度都执行一遍，从而得到了所有的因子矩阵 $U^{(1)}, U^{(2)}, \dots, U^{(N)}$。

&emsp;&emsp;这种方法保证了每个因子矩阵 $U^{(n)}$ 都最优地捕捉了其对应维度上的变化和信息。

**3. 如何计算Tucker分解？—— 高阶正交迭代 (HOOI)**

&emsp;&emsp;与SVD可以通过直接的代数方法求解不同，找到最优的Tucker分解通常是一个非凸优化问题。最常用的算法是**高阶正交迭代（Higher-Order Orthogonal Iteration, HOOI）**，它是一种**交替最小二乘法（Alternating Least Squares, ALS）**的变体。

&emsp;&emsp;HOOI算法的目标是最小化原始张量与重构张量之间的误差，通常用弗罗贝尼乌斯范数的平方来衡量：

$$
\min_{\mathcal{G}, U^{(1)}, \dots, U^{(N)}} \|\mathcal{X} - \mathcal{G} \times_1 U^{(1)} \times_2 \dots \times_N U^{(N)}\|_F^2
$$

&emsp;&emsp;由于因子矩阵 $U^{(n)}$ 是列正交的，这个优化问题可以等价地表示为：

$$
\max_{U^{(1)}, \dots, U^{(N)}} \|\mathcal{X} \times_1 (U^{(1)})^T \times_2 (U^{(2)})^T \dots \times_N (U^{(N)})^T\|_F^2 = \|\mathcal{G}\|_F^2
$$

&emsp;&emsp;这意味着，我们希望通过投影找到能量（范数）最大的核心张量。

&emsp;&emsp;HOOI算法的迭代过程如下：

1. **初始化**：为除 $U^{(1)}$ 之外的所有因子矩阵 $U^{(2)}, \dots, U^{(N)}$ 赋予随机的列正交初始值。

2. **交替更新**：循环迭代更新每一个因子矩阵，直到收敛。在更新第 $n$ 个因子矩阵 $U^{(n)}$ 时，我们固定所有其他的因子矩阵 $U^{(j)} (j \ne n)$。

   - 构建一个中间张量 $\mathcal{Y} = \mathcal{X} \times_1 (U^{(1)})^T \dots \times_{n-1} (U^{(n-1)})^T \times_{n+1} (U^{(n+1)})^T \dots \times_N (U^{(N)})^T$。
   - 对 $\mathcal{Y}$ 进行模态-n展开，得到矩阵 $Y_{(n)}$。
   - 计算 $Y_{(n)}$ 的SVD。
   - 将新的 $U^{(n)}$ 更新为 $Y_{(n)}$ 的前 $R_n$ 个左奇异向量。

3. **收敛判断**：当所有因子矩阵的变化都非常小，或者重构误差不再显著下降时，算法收敛。

4. **计算核心张量**：算法收敛后，使用最终得到的因子矩阵计算核心张量：
   $$
   G=X×1(U(1))T×2(U(2))T⋯×N(U(N))T\mathcal{G} = \mathcal{X} \times_1 (U^{(1)})^T \times_2 (U^{(2)})^T \dots \times_N (U^{(N)})^T
   $$

&emsp;&emsp;这个算法虽然不能保证找到全局最优解，但在实践中通常能得到非常好的结果。

**4. SVD是Tucker分解的特例**

&emsp;&emsp;Tucker分解与SVD之间的关系非常深刻。实际上，**矩阵的SVD是二阶张量Tucker分解的一个特殊情况**。

&emsp;&emsp;考虑一个矩阵 $W \in \mathbb{R}^{m \times n}$，它是一个二阶张量。其Tucker分解为：

$$
W \approx \mathcal{G} \times_1 U^{(1)} \times_2 U^{(2)}
$$

其中 $U^{(1)} \in \mathbb{R}^{m \times R_1}$，$U^{(2)} \in \mathbb{R}^{n \times R_2}$，核心张量 $\mathcal{G} \in \mathbb{R}^{R_1 \times R_2}$。

&emsp;&emsp;如果我们在没有任何截断的情况下（即 $R_1=m, R_2=n$）进行分解，HOOI算法找到的因子矩阵 $U^{(1)}$ 和 $U^{(2)}$ 分别是 $W$ 的左奇异向量矩阵 $U$ 和右奇异向量矩阵 $V$。此时，核心张量 $\mathcal{G}$ 会变成一个对角矩阵，其对角线上的元素就是奇异值 $\sigma_i$。

$$
\mathcal{G} = W \times_1 U^T \times_2 V^T = U^T W V = \Sigma
$$

&emsp;&emsp;于是，Tucker分解的形式 $W = \mathcal{G} \times_1 U \times_2 V$ 就变成了 $W = \Sigma \times_1 U \times_2 V = U \Sigma V^T$，这正是SVD的定义。

&emsp;&emsp;因此，SVD可以看作是这样一个Tucker分解：它的核心张量是**对角化的**。而一般张量的Tucker分解，其核心张量通常是**稠密**的，这反映了高维数据中维度间更复杂的交互关系。

**5. 优点与局限**

&emsp;&emsp;优点：**结构保持**：Tucker分解能够保持数据的高维结构，从而捕捉到矩阵分解方法会丢失的维度间交互信息。**高度灵活性**：它允许每个维度有不同的降维秩（$R_1, R_2, \dots, R_N$），可以根据不同维度的重要性进行灵活的压缩。**可解释性**：因子矩阵 $U^{(n)}$ 提供了对每个维度主要模式的洞察，而核心张量 $\mathcal{G}$ 揭示了这些模式之间的相互作用强度。

&emsp;&emsp;局限：**计算成本高**：HOOI算法涉及对大型中间矩阵进行多次SVD，计算成本可能非常高，尤其是在张量维度和阶数增加时，会面临“**维度灾难**”问题。**秩的选择困难**：如何为每个维度确定最优的秩 $(R_1, R_2, \dots, R_N)$ 是一个核心挑战。与SVD中可以通过观察奇异值衰减来选择秩 $k$ 不同，Tucker分解的秩选择没有统一的、简单的方法。**非唯一性与局部最优**：由于HOOI是迭代算法，其结果可能收敛到局部最优解，而非全局最优。分解的结果也并非严格唯一（可以通过旋转等操作得到不同的但等价的分解）。**核心张量的解释**：虽然因子矩阵相对容易解释，但理解稠密的核心张量 $\mathcal{G}$ 中所有元素的具体含义可能非常复杂。

### CP分解

&emsp;&emsp;CP分解的历史可以追溯到1970年，由两位研究者在不同领域独立提出：Carroll和Chang在心理计量学领域将其命名为**CANDECOMP**（Canonical Decomposition），如今，CP分解已成为张量分解的代名词之一。

&emsp;&emsp;如果说Tucker分解是SVD在保留“交互核心”下的高维推广，那么CP分解则是SVD的**外积求和形式**在更高维度上最直接、最纯粹的延伸。它的目标是将一个高维张量表示为一系列**秩1张量**的加权和。

**1. 什么是CP分解？**

&emsp;&emsp;CP分解的核心思想极其直观和优雅：它假设一个复杂的高维数据体，可以由几个潜在的、基础的“因子组合”线性叠加而成。

&emsp;&emsp;对于一个N阶张量 $\mathcal{X} \in \mathbb{R}^{I_1 \times I_2 \times \dots \times I_N}$，其秩为 $R$ 的CP分解被定义为：

$$
\mathcal{X} \approx \sum_{r=1}^{R} \lambda_r (\mathbf{a}_r^{(1)} \circ \mathbf{a}_r^{(2)} \circ \dots \circ \mathbf{a}_r^{(N)})
$$

其中：

- **$R$** 是分解的秩，代表了我们用来近似原始张量的秩1张量的数量。
- **$\lambda_r$** 是一个标量权重，表示第 $r$ 个秩1分量的重要性。通常，这些权重被吸收到因子向量中，使得分解形式更加简洁。
- **$\mathbf{a}_r^{(n)} \in \mathbb{R}^{I_n}$** 是一个向量，代表第 $r$ 个分量在第 $n$ 个维度上的“特征”或“签名”。
- **$\circ$** 表示向量的**外积**。$N$个向量的外积会生成一个N阶的秩1张量。

&emsp;&emsp;为了便于表示，我们通常将每个维度的所有 $R$ 个因子向量组合成**因子矩阵**：

- $A^{(1)} = [\mathbf{a}_1^{(1)}, \mathbf{a}_2^{(1)}, \dots, \mathbf{a}_R^{(1)}] \in \mathbb{R}^{I_1 \times R}$
- $A^{(2)} = [\mathbf{a}_1^{(2)}, \mathbf{a}_2^{(2)}, \dots, \mathbf{a}_R^{(2)}] \in \mathbb{R}^{I_2 \times R}$
- ...
- $A^{(N)} = [\mathbf{a}_1^{(N)}, \mathbf{a}_2^{(N)}, \dots, \mathbf{a}_R^{(N)}] \in \mathbb{R}^{I_N \times R}$

&emsp;&emsp;吸收权重 $\lambda_r$ 后（通常将 $\lambda_r$ 的值均分或全部归于其中一个因子矩阵的列范数），CP分解可以用一个非常紧凑的“双括号”表示法来写：

$$
\mathcal{X} \approx [\![ A^{(1)}, A^{(2)}, \dots, A^{(N)} ]\!]
$$

&emsp;&emsp;以一个三阶张量（例如 `用户-电影-时间`）为例，其CP分解可以写成：

$$
\mathcal{X} \approx \sum_{r=1}^{R} \mathbf{a}_r \circ \mathbf{b}_r \circ \mathbf{c}_r
$$

其中 $A = [\mathbf{a}_1, \dots, \mathbf{a}_R]$, $B = [\mathbf{b}_1, \dots, \mathbf{b}_R]$, $C = [\mathbf{c}_1, \dots, \mathbf{c}_R]$ 分别是用户、电影和时间维度的因子矩阵。

![CP分解示意图](images/CP.png)

&emsp;&emsp;从元素层面看，这个分解意味着张量中的每一个元素都可以被近似为：

$$
x_{i_1 i_2 \dots i_N} \approx \sum_{r=1}^{R} a_{i_1 r}^{(1)} a_{i_2 r}^{(2)} \dots a_{i_N r}^{(N)}
$$

&emsp;&emsp;这个公式清晰地揭示了CP分解的“平行因子”特性：第 $r$ 个分量完全由每个因子矩阵的第 $r$ 列向量（$\mathbf{a}_r^{(1)}, \mathbf{a}_r^{(2)}, \dots, \mathbf{a}_r^{(N)}$）共同决定，不同分量之间没有像Tucker分解那样的交互核心。

**2. 张量秩（CP秩）**

&emsp;&emsp;在矩阵的世界里，“秩”的概念清晰明了。然而，对于张量，**CP秩**的定义和计算要复杂得多，这也是张量分解区别于矩阵分解的核心特征之一。

**张量的CP秩被定义为能够精确表示该张量所需的最少的秩1张量的数量。**

$$
\text{rank}(\mathcal{X}) = \min \{ R \mid \mathcal{X} = \sum_{r=1}^{R} \mathbf{a}_r^{(1)} \circ \dots \circ \mathbf{a}_r^{(N)} \}
$$

&emsp;&emsp;这个定义引出了几个关键的、有时甚至是反直觉的特性：

1. **计算的困难性**：确定一个给定张量的CP秩是一个**NP-hard**问题。在实践中，我们通常不是去计算精确的秩，而是为低秩近似选择一个合适的秩 $R$。
2. **秩可能大于维度**：与矩阵的秩绝不会超过其行数或列数不同，张量的CP秩可能远大于其任何一个维度的尺寸。例如，一个 $2 \times 2 \times 2$ 的张量，其最大CP秩可以是3。
3. **最佳低秩近似可能不存在**：这是与SVD的Eckart-Young-Mirsky定理最显著的区别。对于某些张量和秩 $R$，可能不存在一个“最佳”的秩-R近似。近似解可能会无限趋近于一个秩大于R的张量。这使得张量低秩近似问题在理论上比矩阵要复杂得多。

**3. CP分解的独特魅力：唯一性**

&emsp;&emsp;CP分解最引人注目的优点之一是其**唯一性**。与SVD（存在符号模糊）和Tucker分解（核心张量和因子矩阵之间存在旋转模糊）不同，在非常宽松的条件下，CP分解的结果是**本质上唯一**的，不存在旋转模糊。

&emsp;&emsp;这意味着，如果一个张量确实可以由一组秩1分量生成，那么CP分解能够恢复出这些原始的分量，唯一的模糊性在于：**列的置换**：可以任意调换秩1分量的顺序。**尺度的缩放**：可以将一个因子向量乘以一个常数 $\alpha$，同时将另一个因子向量除以 $\alpha$，其外积结果不变。

&emsp;&emsp;这种唯一性使得CP分解在需要解释潜在因子或进行盲源分离的应用中极为强大。例如，在化学计量学中，如果混合物光谱是一个张量，CP分解可以唯一地分离出纯净物的光谱（因子向量）及其浓度（另一个因子向量），这是Tucker分解无法直接做到的。

### 非负矩阵分解(NMF)：

&emsp;&emsp;在前面的章节中，我们探讨了SVD、Tucker分解和CP分解等强大的低秩方法。这些方法在数学上优雅且高效，但它们有一个共同的特点：分解出的因子矩阵中通常包含负值。虽然SVD在最小化重构误差方面是**数学上最优**的，但其包含负值的“特征”（即奇异向量）往往缺乏直观的物理解释。例如，用SVD分解一个人脸图像数据库，得到的“特征脸”是一些明暗交错、正负值混合的“幽灵脸”，我们很难将其理解为真实人脸的组成部分。

&emsp;&emsp;在许多现实世界的应用场景中，数据本身就是非负的（如图像像素强度、文档词频、基因表达水平），我们更希望找到一种“**基于部件**”的表示。我们直觉地认为，一个整体是由其各个部分**相加**构成的，而不是通过部件的加减组合。例如，一张人脸是由眼睛、鼻子、嘴巴等部件组合而成，而不是“一张标准脸”加上“半个鼻子”再减去“四分之一只眼睛”。

&emsp;&emsp;为了实现这种更具解释性的、纯粹相加的分解，**非负矩阵分解**应运而生。它是一种带约束的低秩矩阵分解方法，其核心思想是：在分解过程中，强制要求所有因子矩阵的元素都为非负数。

**1. 什么是NMF？**

&emsp;&emsp;非负矩阵分解（NMF）的目标是将一个非负矩阵 $V \in \mathbb{R}^{m \times n}$ 近似分解为两个非负矩阵 $W \in \mathbb{R}^{m \times k}$ 和 $H \in \mathbb{R}^{k \times n}$ 的乘积：

$$
V \approx WH
$$

其中，关键的约束条件是：

$$
W_{ij} \ge 0, \quad H_{ij} \ge 0 \quad \text{for all } i, j
$$

&emsp;&emsp;这里的 $k$ 是我们预先设定的一个正整数，代表了分解的“秩”或“主题数”，通常选择 $k$ 使得 $k \ll \min(m, n)$。

&emsp;&emsp;三个矩阵的含义如下：

- $V$ (原始数据矩阵)：每一列可以代表一个数据样本（例如一张人脸图像拉成的向量，一篇文档的词频向量），每一行代表一个特征（例如一个像素点，一个单词）。
- $W$ (基/特征矩阵)：$W$ 的每一列可以被看作是一个“基向量”或“原型特征”。由于非负性约束，这些基向量往往对应着数据中可被直观理解的“部件”。例如，在人脸数据中，$W$ 的列可能分别代表了眼睛、鼻子、嘴巴等局部特征。
- $H$ (系数/激活矩阵)：$H$ 的每一列给出了如何用 $W$ 中的基向量来线性组合（重构）出 $V$ 中对应的原始数据样本。由于非负性，$H$ 中的系数只能是正的权重，这实现了“纯粹相加”的重构过程。$H$ 矩阵也因此常被视为对原始数据的降维表示。

&emsp;&emsp;这个分解可以看作：

$$
\mathbf{v}_j \approx W \mathbf{h}_j = \sum_{i=1}^{k} \mathbf{w}_i h_{ij}
$$

其中 $\mathbf{v}_j$ 是 $V$ 的第 $j$ 列，$\mathbf{w}_i$ 是 $W$ 的第 $i$ 列，$\mathbf{h}_j$ 是 $H$ 的第 $j$ 列。这个公式清晰地表明，每个数据样本 $\mathbf{v}_j$ 都是由基向量 $\mathbf{w}_i$ 的加权和来表示的，权重就是 $H$ 中的系数。

**3. 如何求解NMF？一个非凸优化问题**

&emsp;&emsp;与SVD可以通过直接的代数运算得到唯一解不同，NMF的求解是一个**非凸优化问题**。这意味着：

1. 通常不存在闭式解。
2. 求解过程需要依赖迭代算法。
3. 算法可能会陷入局部最优解，而非全局最优解。
4. 最终结果依赖于算法的初始值。

&emsp;&emsp;NMF最核心的优化目标是最小化原始矩阵 $V$ 与重构矩阵 $WH$ 之间的差异。衡量这种差异的目标函数通常有两种：

1. **欧几里得距离（弗罗贝尼乌斯范数）**：

$$
   min⁡W,H∥V−WH∥F2=min⁡W,H∑i,j(Vij−(WH)ij)2s.t.W,H≥0\min_{W, H} \|V - WH\|_F^2 = \min_{W, H} \sum_{i,j} (V_{ij} - (WH)_{ij})^2 \quad \text{s.t.} \quad W, H \ge 0
$$

2. **广义Kullback-Leibler (KL)散度**：

$$
min⁡W,HDKL(V∣∣WH)=min⁡W,H∑i,j(Vijlog⁡Vij(WH)ij−Vij+(WH)ij)s.t.W,H≥0\min_{W, H} D_{KL}(V || WH) = \min_{W, H} \sum_{i,j} \left(V_{ij} \log\frac{V_{ij}}{(WH)_{ij}} - V_{ij} + (WH)_{ij}\right) \quad \text{s.t.} \quad W, H \ge 0
$$

   KL散度在处理基于计数的稀疏数据（如文本词频）时往往能取得更好的效果。

&emsp;&emsp;最著名且广泛使用的求解算法是Lee和Seung提出的**乘法更新法则**。该算法也是一种交替优化的思想，即固定一个矩阵，更新另一个。针对最小化欧氏距离的目标，其更新规则非常简洁：

&emsp;&emsp;首先随机初始化非负的 $W$ 和 $H$，然后反复迭代以下两步直至收敛：

1. **更新 $H$（固定 $W$）**:

$$
Haj←Haj(WTV)aj(WTWH)ajH_{aj} \leftarrow H_{aj} \frac{(W^T V)_{aj}}{(W^T W H)_{aj}}
$$

2. **更新 $W$（固定 $H$）**:

$$
Wia←Wia(VHT)ia(WHHT)iaW_{ia} \leftarrow W_{ia} \frac{(V H^T)_{ia}}{(W H H^T)_{ia}}
$$

&emsp;&emsp;这些更新规则的巧妙之处在于，它们保证了如果 $W$ 和 $H$ 的初始值是非负的，那么在每次迭代之后它们将**保持非负**。从直观上看，更新的乘法因子是一个分数：分子 $(W^T V)$ 代表了当前基与原始数据的“期望关联”，而分母 $(W^T W H)$ 代表了当前基与重构数据的“实际关联”。通过这个比例来调整系数，使得重构结果不断逼近原始数据。

**4. 优势与局限性**

&emsp;&emsp;优点：**强大的可解释性**：这是NMF最核心的优点，分解出的因子直观、易于理解，符合人类对“整体由部分构成”的认知。**天然的稀疏性**：非负约束常常会引导出稀疏的系数矩阵H，这意味着每个样本仅由少数几个基来表示，这符合许多现实场景的假设。**易于实现的算法**：乘法更新法则简单直观，容易实现。

&emsp;&emsp;局限性：**非凸性**：结果依赖于初始化，可能收敛到局部最优，需要多次运行以寻找一个较好的解。**秩k的选择**：如何确定最佳的分解秩 $k$ 是一个难题，通常需要领域知识或交叉验证等方法来辅助判断。**非唯一性**：对于同一个矩阵，可能存在多个不同的NMF分解结果，这给结果的稳定性带来挑战。

## 为何要进行低秩分解 (Why Low-Rank Decomposition?)

&emsp;&emsp;在理解了低秩分解是什么以及如何实现之后，一个自然的问题是：我们为什么要使用它？低秩分解带来的好处是多方面的，它不仅是理论上优雅的数学工具，更在实际应用中，尤其是在大规模模型优化中，展现出巨大的价值。主要优势可以概括为以下几点：

### 显著减少模型参数

&emsp;&emsp;这是低秩分解最直接、最显而易见的好处。在深度学习中，全连接层和卷积层的权重通常以矩阵形式存储。这些权重矩阵，特别是在大型模型中，占据了大量的存储空间。

&emsp;&emsp;通过将权重矩阵 $W \in \mathbb{R}^{m \times n}$ 分解为 $U \in \mathbb{R}^{m \times k}$ 和 $V^T \in \mathbb{R}^{k \times n}$，我们将参数存储从 $m \times n$ 减少到了 $k(m+n)$。当 $k$ 远小于 $m$ 和 $n$ 时，这种减少是巨大的。

**示例**：考虑一个大型语言模型中的一个全连接层，输入维度 $n_{in}=4096$，输出维度 $n_{out}=4096$。

* 原始参数量：$4096 \times 4096 \approx 16.8$ 百万。
* 假设该权重矩阵可以被一个秩 $k=256$ 的矩阵很好地近似。
* 分解后参数量：$256 \times (4096 + 4096) = 256 \times 8192 \approx 2.1$ 百万。
* 仅通过对这一层进行低秩分解，就实现了大约 **8倍** 的参数压缩。

&emsp;&emsp;这种参数量的减少使得在资源受限的设备（如手机、边缘计算设备）上部署大型模型成为可能。

### 大幅提升计算效率

&emsp;&emsp;模型的计算量，尤其是推理速度，是衡量其可用性的关键指标。低秩分解通过将一次昂贵的大矩阵乘法替换为两次廉价的小矩阵乘法，显著降低了计算复杂度。

&emsp;&emsp;对于输入 $x \in \mathbb{R}^n$，原始计算 $y = Wx$ 的浮点运算次数（FLOPs）约为 $2 \times m \times n$。使用分解后的矩阵 $W \approx UV^T$，计算变为 $y = U(V^T x)$：

1. 计算 $z = V^T x$：FLOPs 约为 $2 \times k \times n$。
2. 计算 $y = Uz$：FLOPs 约为 $2 \times k \times m$。
3. 总 FLOPs 约为 $2k(m+n)$。

&emsp;&emsp;计算复杂度的加速比约为：

$$
\text{Speed-up} \approx \frac{2mn}{2k(m+n)} = \frac{mn}{k(m+n)}
$$

当 $m=n=1000, k=50$ 时，加速比约为 $\frac{1000^2}{50(2000)} = 10$ 倍。

&emsp;&emsp;这种计算效率的提升对于需要实时响应的应用（如实时视频分析、在线推荐）至关重要。

### 揭示数据潜在结构与特征提取

&emsp;&emsp;低秩分解不仅仅是一种压缩技巧，它还是一种强大的分析工具，能够帮助我们理解数据背后的潜在结构。

&emsp;&emsp;分解后的矩阵 $U$ 和 $V$ 可以被解释为：
* $V$ 的行向量（$V^T$的列向量）构成了描述数据的一个 $k$ 维“潜在特征空间”的基。
* $U$ 的行向量是原始数据点在这些潜在特征上的“坐标”或“嵌入”。

**经典案例：潜在语义分析 (Latent Semantic Analysis, LSA)**

&emsp;&emsp;在自然语言处理中，我们可以构建一个“词语-文档”矩阵 $W$，其中 $W_{ij}$ 表示词语 $i$ 在文档 $j$ 中出现的频率。这个矩阵通常非常巨大且稀疏。对其进行低秩分解，得到的潜在特征可以被解释为“主题”或“概念”。例如，一个特征向量可能对“船”、“水”、“海洋”等词语有高权重，而对“汽车”、“道路”、“轮胎”等词语有低权重，从而捕捉到了“航海”这一主题。通过将词语和文档都投影到这个低维的“主题空间”，我们可以更好地衡量它们之间的语义相似度，即使它们没有共享完全相同的词语。

### 实现数据去噪与鲁棒性提升

&emsp;&emsp;SVD低秩近似的另一个重要作用是**去噪**。通常认为，数据中的主要结构和信号由大的奇异值所承载，而小的奇异值往往与测量噪声、随机波动等不稳定因素相关。

&emsp;&emsp;通过在SVD分解后舍弃这些小的奇异值（即 $i > k$ 的 $\sigma_i$），我们相当于在数据中做了一次滤波，保留了最重要、最稳定的主成分，而滤除了大部分噪声。这使得分解后的模型表示更加鲁棒，对输入数据的微小扰动不那么敏感，有时甚至可以提升模型的泛化能力。

&emsp;&emsp;比如这里，当我们把一幅图用扫描仪扫进电脑中时，会在图像中引入一些瑕疵点（通常称为噪声）（左）。我们将数据用15×25的矩阵来表示，并进行奇异值分解，我们假设其他的部分都是图像中的噪声引起的，只保留前三项奇异值做近似，这样就可以改善图像质量。

![低秩分解去噪示例](images/quzao.png)

&emsp;&emsp;综上所述，低秩分解通过利用数据内在的冗余性，提供了一套强大的工具集，以实现模型压缩、计算加速、特征提取和数据去噪等多重目标。它之所以成为模型优化领域不可或缺的技术，正是因为它在理论的深刻性和实践的有效性之间取得了完美的平衡。

## 秩的选择之道

&emsp;&emsp;在前面几节中，我们深入探讨了SVD、Tucker分解、CP分解和NMF等核心的低秩分解方法。在所有这些方法中，我们都遇到了一个共同的、至关重要参数：**秩** $k$ （或对于Tucker分解的多线性秩 $(R_1, \dots, R_N)$）。这个参数决定了我们用多少个“成分”或“主题”来近似原始数据。

&emsp;&emsp;秩 $k$ 的选择是一个典型的模型选择问题，它直接影响到分解结果的质量和可解释性。这是一个微妙的平衡艺术：

- **如果 $k$ 太小**，模型将过于简化，无法捕捉到数据中重要的结构和信息，导致**欠拟合**。重构误差会很大，我们可能会丢失宝贵的信号。
- **如果 $k$ 太大**，模型将过于复杂。它不仅会捕捉数据中的信号，还会开始对噪声进行建模，导致**过拟合**。这会降低模型的泛化能力，分解出的因子也会变得冗余、不稳定，甚至难以解释。对于CP分解，过高的秩还可能引发模型退化问题。

&emsp;&emsp;那么，我们该如何在这个“信号”与“噪声”的十字路口，做出明智的选择呢？遗憾的是，**不存在一个放之四海而皆准的“万能公式”**。但在长期的实践中，研究者们发展出了一系列行之有效的方法和策略，可以从不同角度为我们提供指引。这些方法大致可以分为三类：启发式方法、量化指标评估法、以及基于任务和稳定性的方法。

**1. 启发式：寻找“拐点”**

&emsp;&emsp;这类方法直观、易于实施，通常是我们选择秩的第一步，帮助我们快速缩小备选范围。对于像NMF这样以可解释性为目标的方法，直接观察分解出的因子是一种非常有效的方法。我们可以尝试一系列的 $k$ 值，然后将每个 $k$ 值对应的基矩阵 $W$（或因子矩阵 $A^{(n)}$）的列向量可视化。

- 当 $k$ 较小时，因子通常是清晰、可辨认、有意义的“部件”或“主题”。
- 随着 $k$ 的增加，新的因子可能依然有意义。
- 但当 $k$ 变得过大时，新分解出的因子可能会变得模糊、充满噪声，或者与已有的因子高度相似（冗余）。当你发现分解结果开始出现这些“坏”的因子时，说明当前的 $k$ 可能已经过高了。

**2. 量化指标评估法：让数字说话**

&emsp;&emsp;这类方法试图通过更客观的数值指标来评估不同秩 $k$ 的优劣。

**a) 重构误差曲线**

&emsp;&emsp;我们可以绘制重构误差随 $k$ 变化的曲线。对于给定的 $k$，我们计算出近似矩阵 $V_k$，然后计算其与原始矩阵 $V$ 的误差，例如相对弗罗贝尼乌斯范数：

$$
\text{Error}(k) = \frac{\|V - V_k\|_F}{\|V\|_F}
$$

&emsp;&emsp;这条误差曲线必然是单调递减的。我们可以寻找曲线斜率变化最显著的“肘部”。当增加 $k$ 不再能显著降低误差时，说明再增加的成分主要是在拟合噪声，此时的 $k$ 就是一个不错的选择。

**b) 残差分析**

&emsp;&emsp;检查**残差矩阵 $E = V - V_k$** 也是一个好方法。

- 如果 $k$ 合适，残差矩阵 $E$ 应该看起来像随机噪声，不包含任何明显的结构或模式。
- 如果 $k$ 过小，残差矩阵 $E$ 中仍然会残留原始数据中的结构化信息。你可以通过可视化残差矩阵或对其进行统计分析来判断。

**3. 基于任务和稳定性的方法**

&emsp;&emsp;对于NMF和CP分解这类解不唯一的迭代算法，结果的**稳定性**是衡量模型好坏的一个关键标准，也为秩选择提供了有力的依据。其核心思想是：**一个好的、非过拟合的模型，对于数据的微小扰动应该是鲁棒的。**

**a) 模型一致性/稳定性评估**

&emsp;&emsp;对于一个给定的秩 $k$：

1. 多次运行分解算法（例如，使用不同的随机初始值）。
2. 比较每次运行得到的因子矩阵。例如，对于NMF得到的多个 $W$ 和 $H$，我们可以计算它们之间的一致性（如使用聚类指标或相关性分析）。
3. 如果对于某个 $k$，每次运行都得到高度相似、稳定的结果，说明这个 $k$ 是一个鲁棒的选择。
4. 如果对于某个 $k$，每次运行结果都大相径庭，说明模型不稳定，很可能已经过拟合或者 $k$ 的选择不合适。

&emsp;&emsp;研究者们已经提出了具体的指标来量化这种稳定性，例如**共识矩阵**。通过对不同的 $k$ 值绘制其稳定性得分，我们可以找到最稳定的秩。

**b) 预测性能/下游任务表现**

&emsp;&emsp;在许多机器学习应用中，低秩分解是作为特征提取或降维的预处理步骤。在这种情况下，最佳的秩 $k$ 是那个能在**下游任务（如分类、回归、聚类）中取得最好性能**的 $k$。

&emsp;&emsp;我们可以使用**交叉验证**来确定最优的 $k$：

1. 将数据划分为训练集和验证集。
2. 对于一系列候选的 $k$ 值，在训练集上进行低秩分解。
3. 利用分解得到的低维表示，在下游任务上训练模型，并在验证集上评估其性能。
4. 选择在验证集上性能最好的那个 $k$ 值。

&emsp;&emsp;这种方法将秩的选择与最终的应用目标直接挂钩，是最具实用主义的策略，但计算成本也相对较高。

## 模型低秩分解对象

**1. 权重矩阵**

&emsp;&emsp;这是最经典、最直接的应用目标，主要用于**减小模型的静态存储体积**。现代神经网络中，大量的参数集中在全连接层和卷积层。

- **全连接层**：其权重本身就是一个巨大的矩阵 $W \in \mathbb{R}^{m \times n}$。通过低秩分解，我们可以将其替换为两个更小的矩阵 $A \in \mathbb{R}^{m \times k}$ 和 $B \in \mathbb{R}^{k \times n}$ 的乘积 ($W \approx AB$)。
- **卷积层**：其四维权重张量 $(C_{out}, C_{in}, K_h, K_w)$ 也可以通过张量分解（如Tucker分解或CP分解）进行低秩近似，从而实现参数的有效压缩。

**2. 键值缓存**

&emsp;&emsp;在Transformer架构（如GPT系列模型）的自回归生成任务中，为了避免重复计算，系统需要缓存并重用过去所有token的“键（Key）”和“值（Value）”，这构成了所谓的**KV-Cache**。通过对庞大的KV-Cache矩阵进行在线的低秩近似（例如，仅存储其最重要的奇异值分量），可以用极小的代价保留其核心信息。这种方法被称为**缓存压缩**，它能大幅降低推理时的峰值显存占用，从而支持更长序列的生成或在同等硬件上实现更高的吞吐量。

**3. 其他目标**

- **词嵌入层**：对于拥有巨大词汇表的模型，词嵌入层本质上是一个大型的查找表（矩阵），同样是低秩分解的绝佳对象，可以有效压缩模型大小。
- **梯度矩阵** ：在分布式训练场景中，各计算节点间需要频繁交换梯度信息。通过对梯度矩阵进行低秩分解，可以显著降低通信带宽的需求，从而加速整体训练过程。

## 实践
&emsp;&emsp;在本节中，我们将通过实际的代码示例，深入探讨低秩分解在模型压缩中的应用。我们将从简单的矩阵分解开始，逐步过渡到复杂的张量分解方法，并展示如何在实际模型中实现这些技术。

### 不同低秩分解方法实践

低秩分解是一种常用的模型压缩技术，其核心思想是将权重张量分解为多个低秩矩阵的乘积，从而减少参数量和计算量。本实践将展示几种常见的低秩分解方法。完整代码见[不同低秩分解方法实践](https://github.com/datawhalechina/awesome-compression/blob/main/docs/notebook/ch07/1.methods_of_low-rank_decomposition.ipynb)。

下面我们将通过实际例子和可视化方式，直观地展示不同低秩分解方法的原理和效果。


```python
import torch
import numpy as np
import matplotlib.pyplot as plt
import tensorly as tl
from tensorly.decomposition import tucker
from PIL import Image
import warnings
warnings.filterwarnings("ignore")

# 设置默认字体
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']
plt.rcParams['font.size'] = 12
plt.rcParams['axes.unicode_minus'] = False  # 解决负号显示问题

# 设置种子以保证结果可复现
torch.manual_seed(42)
np.random.seed(42)
```

奇异值分解(Singular Value Decomposition, SVD)是一种常用的矩阵分解方法，可以将任意矩阵分解为三个矩阵的乘积：A = USV^T，其中U和V是正交矩阵，S是对角矩阵，对角线上的元素称为奇异值。

在神经网络压缩中，我们可以通过截断小的奇异值来实现低秩近似，从而减少参数量和计算量。


```python
def plot_matrix(matrix, title, colorbar=True):
    """可视化矩阵"""
    fig, ax = plt.subplots(figsize=(8, 6))
    im = ax.imshow(matrix.detach().numpy(), cmap='viridis')
    ax.set_title(title)
    ax.set_xlabel('Column Index')
    ax.set_ylabel('Row Index')
    
    # 添加颜色条
    if colorbar:
        plt.colorbar(im, ax=ax, label='Value')
    
    # 显示图像
    plt.tight_layout()
    plt.show()

def plot_singular_values(s, threshold=None, percentage=None):
    """可视化奇异值及其能量分布"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
    
    # 绘制奇异值
    x_indices = np.arange(1, len(s) + 1)
    ax1.plot(x_indices, s.numpy(), 'o-', markersize=8)
    ax1.set_title('Singular Values')
    ax1.set_xlabel('Index')
    ax1.set_ylabel('Value')
    ax1.grid(True)

    if threshold is not None:
        ax1.axhline(y=threshold, color='r', linestyle='--', label=f'Threshold: {threshold:.4f}')
        ax1.legend()
    
    # 绘制奇异值的累积能量
    energy = torch.cumsum(s**2, dim=0) / torch.sum(s**2)
    ax2.plot(x_indices, energy.numpy(), 'o-', markersize=8)
    ax2.set_title('Cumulative Energy')
    ax2.set_xlabel('Number of Singular Values')
    ax2.set_ylabel('Energy Percentage')
    ax2.grid(True)
    
    if percentage is not None:
        # 找到第一个超过百分比的索引
        idx = torch.where(energy >= percentage)[0][0].item()
        ax2.axhline(y=percentage, color='r', linestyle='--', 
                    label=f'{percentage*100:.0f}% Energy at k={idx+1}')
        ax2.axvline(x=idx+1, color='g', linestyle='--')
        ax2.legend()
    
    plt.tight_layout()
    plt.show()
    
    return energy
```


```python
# 创建一个随机矩阵作为原始权重
original_matrix = torch.rand(10, 8)
plot_matrix(original_matrix, 'Original Matrix')

# 对矩阵进行SVD分解
U, S, V = torch.svd(original_matrix)

# 展示奇异值
plot_singular_values(S, percentage=0.9)
```

输出结果如下：

![矩阵可视化示例图](images/1.methods_of_low-rank_decomposition_4_1.png)
    
![奇异值及其累积能量示例图](images/1.methods_of_low-rank_decomposition_4_2.png)
    
```
    tensor([0.8245, 0.9070, 0.9520, 0.9747, 0.9881, 0.9966, 0.9997, 1.0000])
```

上面展示了一个随机生成的10×8矩阵及其奇异值分布。右侧图表显示了累积能量占比，可以看到只需要保留前几个奇异值就能保留矩阵大部分的信息。保留90%的能量只需要少量的奇异值。

接下来，我们将演示如何使用SVD进行低秩近似，并比较不同秩下的近似效果。


```python
def low_rank_approximation(U, S, V, rank):
    """根据指定的秩进行低秩近似"""
    return U[:, :rank] @ torch.diag(S[:rank]) @ V[:, :rank].T

def compare_approximations(original, U, S, V, ranks):
    """比较不同秩下的近似效果，并添加原始矩阵作为参考"""
    # 添加原始矩阵
    all_displays = ranks + ['Original']
    
    fig, axes = plt.subplots(2, len(all_displays), figsize=(4*len(all_displays), 8))
    
    for i, rank in enumerate(ranks):
        # 低秩近似
        approx = low_rank_approximation(U, S, V, rank)
        
        # 计算误差
        error = torch.abs(original - approx)
        
        # 绘制近似结果
        im1 = axes[0, i].imshow(approx.detach().numpy(), cmap='viridis')
        axes[0, i].set_title(f'Rank-{rank} Approximation')
        axes[0, i].set_xlabel('Column Index')
        axes[0, i].set_ylabel('Row Index')
        plt.colorbar(im1, ax=axes[0, i])
        
        # 绘制误差
        im2 = axes[1, i].imshow(error.detach().numpy(), cmap='hot')
        axes[1, i].set_title(f'Error (Rank-{rank})')
        axes[1, i].set_xlabel('Column Index')
        axes[1, i].set_ylabel('Row Index')
        plt.colorbar(im2, ax=axes[1, i], label='Absolute Error')
    
    # 添加原始矩阵
    im_orig = axes[0, -1].imshow(original.detach().numpy(), cmap='viridis')
    axes[0, -1].set_title('Original Matrix')
    axes[0, -1].set_xlabel('Column Index')
    axes[0, -1].set_ylabel('Row Index')
    plt.colorbar(im_orig, ax=axes[0, -1])
    
    # 原始矩阵与自身的误差为零
    zeros = torch.zeros_like(original)
    im_error = axes[1, -1].imshow(zeros.detach().numpy(), cmap='hot')
    axes[1, -1].set_title('Error (Original)')
    axes[1, -1].set_xlabel('Column Index')
    axes[1, -1].set_ylabel('Row Index')
    plt.colorbar(im_error, ax=axes[1, -1], label='Absolute Error')
    
    plt.tight_layout()
    plt.show()

# 比较不同秩下的近似效果
ranks = [1, 3, 5, 7]
compare_approximations(original_matrix, U, S, V, ranks)
```

![不同秩下的近似效果](images/1.methods_of_low-rank_decomposition_7_0.png)

上图展示了不同秩下的低秩近似效果，并在最右侧添加了原始矩阵作为参考。随着秩的增加，近似效果逐渐提高，误差逐渐减小。当秩达到原始矩阵的秩时，近似误差为零。

在实际应用中，我们可以根据需要的精度和计算资源选择合适的秩。一般来说，保留90%-95%的能量通常是一个不错的选择。


```python
# 计算不同秩下的压缩率和误差
def compression_analysis(original, U, S, V):
    """分析不同秩下的压缩率和误差"""
    original_params = original.numel()
    max_rank = min(original.shape)
    
    ranks = list(range(1, max_rank + 1))
    compression_ratios = []
    relative_errors = []
    
    for r in ranks:
        # 低秩分解后的参数量
        decomposed_params = U[:, :r].numel() + S[:r].numel() + V[:, :r].numel()
        
        # 压缩率 = 原参数量 / 分解后参数量
        compression_ratio = original_params / decomposed_params
        
        # 低秩近似
        approx = low_rank_approximation(U, S, V, r)
        
        # 相对误差 
        relative_error = torch.norm(original - approx, p='fro') / torch.norm(original, p='fro')
        
        compression_ratios.append(compression_ratio)
        relative_errors.append(relative_error.item())
    
    # 可视化结果
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
    
    ax1.plot(ranks, compression_ratios, 'o-', markersize=8)
    ax1.set_title('Compression Ratio vs Rank')
    ax1.set_xlabel('Rank')
    ax1.set_ylabel('Compression Ratio')
    ax1.grid(True)
    
    ax2.plot(ranks, relative_errors, 'o-', markersize=8)
    ax2.set_title('Relative Error vs Rank')
    ax2.set_xlabel('Rank')
    ax2.set_ylabel('Relative Error')
    ax2.grid(True)
    
    plt.tight_layout()
    plt.show()

# 分析不同秩下的压缩率和误差
compression_analysis(original_matrix, U, S, V)
```

![不同秩下的压缩率和误差](images/1.methods_of_low-rank_decomposition_10_0.png)
    

上图左侧显示了不同秩下的压缩率，可以看到当秩较小时，压缩率很高，但随着秩的增加，压缩率迅速下降。右侧显示了相对误差，随着秩的增加，相对误差逐渐减小。

低秩近似的关键在于找到合适的秩，既能保证足够的压缩率，又能控制误差在可接受范围内。

**SVD在图像压缩中的直观展示**。SVD不仅可以用于神经网络压缩，还可以直接应用于图像压缩。下面我们将展示如何使用SVD对灰度图像进行压缩，并比较不同秩下的压缩效果。


```python
def get_image():
    return Image.open("./Datawhale.jpg")

def compress_image_with_svd(image, ranks):
    """使用SVD对图像进行压缩"""
    # 转换为灰度图像并转为PyTorch张量
    if image.mode != 'L':
        image = image.convert('L')
    img_array = np.array(image)
    img_tensor = torch.tensor(img_array, dtype=torch.float32)
    
    # 对图像进行SVD分解
    U, S, V = torch.svd(img_tensor)
    
    # 创建不同秩下的近似图像
    approximations = []
    for r in ranks:
        approx = low_rank_approximation(U, S, V, r)
        approximations.append(approx)
    
    return img_tensor, U, S, V, approximations

def plot_image_compression(original, approximations, ranks, percentage=None):
    """可视化图像压缩效果"""
    n_ranks = len(ranks)
    fig, axes = plt.subplots(2, n_ranks + 1, figsize=(4*(n_ranks + 1), 8))
    
    # 显示原始图像
    axes[0, 0].imshow(original.numpy(), cmap='gray')
    axes[0, 0].set_title('Original Image')
    axes[0, 0].axis('off')
    
    # 显示原始图像与自身的误差
    axes[1, 0].imshow(torch.zeros_like(original).numpy(), cmap='hot')
    axes[1, 0].set_title('Error (Original)')
    axes[1, 0].axis('off')
    
    # 显示不同秩下的近似图像和误差
    for i, (approx, rank) in enumerate(zip(approximations, ranks)):
        # 确保像素值在合理范围内
        approx_clamped = torch.clamp(approx, 0, 255)
        
        # 显示近似图像
        axes[0, i+1].imshow(approx_clamped.numpy(), cmap='gray')
        axes[0, i+1].set_title(f'Rank-{rank} Approximation')
        axes[0, i+1].axis('off')
        
        # 计算并显示误差
        error = torch.abs(original - approx_clamped)
        axes[1, i+1].imshow(error.numpy(), cmap='hot')
        axes[1, i+1].set_title(f'Error (Rank-{rank})')
        axes[1, i+1].axis('off')
    
    plt.tight_layout()
    plt.show()
    
    # 如果提供了百分比，显示SVD能量分布
    if percentage is not None:
        plot_singular_values(S, percentage=percentage)
    
    # 显示压缩率和保留信息比例
    max_rank = min(original.shape)
    compression_info = []
    for r in ranks:
        # 原始参数量
        original_params = original.numel()
        
        # 压缩后参数量
        compressed_params = U[:, :r].numel() + S[:r].numel() + V[:, :r].numel()
        
        # 压缩率
        compression_ratio =  ( original_params - compressed_params ) / original_params
        
        # 保留能量比例
        energy_ratio = torch.sum(S[:r]**2) / torch.sum(S**2)
        
        compression_info.append((r, compression_ratio, energy_ratio))
    
    # 显示压缩信息
    print("Rank\tCompression Ratio\tEnergy Preserved")
    print("----\t-----------------\t----------------")
    for r, ratio, energy in compression_info:
        print(f"{r}\t{ratio:.2f}\t\t\t{energy:.4f}")

# 下载并压缩经典测试图像
img = get_image()


# 调整图像大小以加快计算速度
img = img.resize((256, 256))

# 设置不同的秩
ranks = [5, 10, 20, 50]

# 使用SVD压缩图像
original, U, S, V, approximations = compress_image_with_svd(img, ranks)

# 可视化压缩效果
plot_image_compression(original, approximations, ranks, percentage=0.9)
```
 
![SVD压缩图像示例图](images/1.methods_of_low-rank_decomposition_14_0.png)
       
![奇异值及其累积能量示例图](images/1.methods_of_low-rank_decomposition_14_1.png)
```    
    Rank	Compression Ratio	Energy Preserved
    ----	-----------------	----------------
    5	0.96			0.9863
    10	0.92			0.9941
    20	0.84			0.9981
    50	0.61			0.9997
```

上图展示了使用SVD对经典测试图像进行压缩的效果。随着保留秩的增加，图像质量逐渐提高，误差逐渐减小。

从数据中可以看出，即使只保留少量的奇异值，也能保留图像的大部分信息。例如，使用秩为50的近似，已经能够很好地重建原始图像，同时实现了显著的数据压缩。

这种图像压缩方法是SVD在实际应用中的一个典型例子，也直观地展示了低秩近似的强大之处。

**高阶张量的Tucker分解**。Tucker分解是一种高阶张量分解方法，可以看作是SVD在高阶张量上的推广。对于一个三阶张量X，Tucker分解可以表示为X ≈ G ×₁ A ×₂ B ×₃ C，其中G是核心张量，A、B、C是因子矩阵，×ᵢ表示第i模式的张量-矩阵乘积。

Tucker分解可以捕捉张量中的多维相关性，是一种更灵活的张量分解方法。


```python
def visualize_3d_tensor(tensor, title, alpha=0.8):
    """
    使用3D散点图可视化三阶张量，点的大小和颜色表示值的大小
    """
    fig = plt.figure(figsize=(10, 8))
    ax = fig.add_subplot(111, projection='3d')
    
    # 获取张量的尺寸和数值
    dim1, dim2, dim3 = tensor.shape
    tensor_np = tensor.detach().numpy()
    
    # 创建坐标网格
    x, y, z = np.meshgrid(np.arange(dim1), np.arange(dim2), np.arange(dim3), indexing='ij')
    
    # 展平数组
    x_flat = x.flatten()
    y_flat = y.flatten()
    z_flat = z.flatten()
    values_flat = tensor_np.flatten()
    
    # 过滤掉接近零的值以减少视觉混乱
    threshold = np.percentile(np.abs(values_flat), 70)
    mask = np.abs(values_flat) >= threshold
    
    x_filtered = x_flat[mask]
    y_filtered = y_flat[mask]
    z_filtered = z_flat[mask]
    values_filtered = values_flat[mask]
    
    # 设置点的大小和颜色
    sizes = 200 * (np.abs(values_filtered) / np.max(np.abs(values_filtered)))
    colors = values_filtered
    
    # 绘制散点图
    scatter = ax.scatter(x_filtered, y_filtered, z_filtered, 
                        c=colors, s=sizes, alpha=alpha, cmap='viridis')
    
    # 设置标题和轴标签
    ax.set_title(f'{title}\nShape: {tensor.shape}')
    ax.set_xlabel('Dimension 1')
    ax.set_ylabel('Dimension 2')
    ax.set_zlabel('Dimension 3')
    
    # 添加颜色条
    plt.colorbar(scatter, ax=ax, label='Value', shrink=0.8)
    
    plt.tight_layout()
    plt.show()

def tucker_decomposition(tensor, ranks):
    """对张量进行Tucker分解"""
    tensor_np = tensor.detach().numpy()
    tl.set_backend('numpy')
    
    # 进行Tucker分解
    core, factors = tucker(tensor_np, rank=ranks)
    
    # 转回PyTorch张量
    core_tensor = torch.tensor(core, dtype=torch.float32)
    factor_matrices = [torch.tensor(factor, dtype=torch.float32) for factor in factors]
    
    return core_tensor, factor_matrices

def reconstruct_from_tucker(core_tensor, factor_matrices):
    """从Tucker分解重建张量"""
    core_np = core_tensor.detach().numpy()
    factors_np = [factor.detach().numpy() for factor in factor_matrices]
    
    tl.set_backend('numpy')
    reconstructed_np = tl.tucker_to_tensor((core_np, factors_np))
    
    return torch.tensor(reconstructed_np, dtype=torch.float32)

# 创建一个三阶张量
original_tensor = torch.randn(6, 8, 5) * 2 + 1

# 可视化原始张量
visualize_3d_tensor(original_tensor, 'Original Tensor')

# 设置Tucker分解的秩
tucker_ranks = [3, 4, 3]

# 进行Tucker分解
core_tensor, factor_matrices = tucker_decomposition(original_tensor, tucker_ranks)

# 可视化核心张量
visualize_3d_tensor(core_tensor, 'Core Tensor')

# 重建张量
reconstructed_tensor = reconstruct_from_tucker(core_tensor, factor_matrices)

# 可视化重建张量
visualize_3d_tensor(reconstructed_tensor, 'Reconstructed Tensor')

# 计算压缩效果
original_params = original_tensor.numel()
compressed_params = core_tensor.numel() + sum(f.numel() for f in factor_matrices)
compression_ratio = original_params / compressed_params
relative_error = torch.norm(original_tensor - reconstructed_tensor) / torch.norm(original_tensor)

print(f'原始参数量: {original_params}')
print(f'压缩后参数量: {compressed_params}')
print(f'压缩率: {compression_ratio:.2f}')
print(f'相对误差: {relative_error.item():.6f}')
```

![原始张量可视化示例图](images/1.methods_of_low-rank_decomposition_18_0.png)
    
![核心张量可视化示例图](images/1.methods_of_low-rank_decomposition_18_1.png)
   
![重建张量可视化示例图](images/1.methods_of_low-rank_decomposition_18_2.png)
    

```
    原始参数量: 240
    压缩后参数量: 101
    压缩率: 2.38
    相对误差: 0.686360
```

上图展示了Tucker分解的三维可视化效果。原始张量、核心张量和重建张量都用散点图表示，其中点的大小表示数值的绝对值大小，颜色表示数值的正负和大小。

可以看到核心张量比原始张量更小更稀疏，但重建张量能够很好地近似原始张量的结构。Tucker分解通过将高维张量分解为低维的核心张量和因子矩阵，实现了有效的数据压缩。


**CP分解**。CP分解是另一种重要的张量分解方法，它将一个三阶张量分解为有限个秩1张量的和。与Tucker分解不同，CP分解没有核心张量，而是直接将张量表示为因子矩阵的外积和。



```python
from tensorly.decomposition import parafac

def cp_decomposition(tensor, rank):
    """对张量进行CP分解"""
    tensor_np = tensor.detach().numpy()
    tl.set_backend('numpy')
    
    # 进行CP分解
    weights, factors = parafac(tensor_np, rank=rank)
    
    # 转回PyTorch张量
    weights_tensor = torch.tensor(weights, dtype=torch.float32)
    factor_matrices = [torch.tensor(factor, dtype=torch.float32) for factor in factors]
    
    return weights_tensor, factor_matrices

def reconstruct_from_cp(weights, factor_matrices):
    """从CP分解重建张量"""
    weights_np = weights.detach().numpy()
    factors_np = [factor.detach().numpy() for factor in factor_matrices]
    
    tl.set_backend('numpy')
    reconstructed_np = tl.cp_to_tensor((weights_np, factors_np))
    
    return torch.tensor(reconstructed_np, dtype=torch.float32)

def visualize_cp_factors(factor_matrices, weights):
    """可视化CP分解的因子矩阵"""
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))
    
    for i, factor in enumerate(factor_matrices):
        # 显示因子矩阵，每列对应一个成分
        im = axes[i].imshow(factor.detach().numpy(), cmap='RdBu_r', aspect='auto')
        axes[i].set_title(f'Factor Matrix {i+1}')
        axes[i].set_xlabel('Component')
        axes[i].set_ylabel(f'Dimension {i+1}')
        plt.colorbar(im, ax=axes[i])
    
    plt.tight_layout()
    plt.show()

def compare_cp_ranks(original_tensor, rank_list):
    """比较不同秩下的CP分解效果"""
    errors = []
    compression_ratios = []
    original_params = original_tensor.numel()
    
    fig, axes = plt.subplots(2, len(rank_list), figsize=(4*len(rank_list), 8))
    
    for i, rank in enumerate(rank_list):
        # 进行CP分解
        weights, factors = cp_decomposition(original_tensor, rank)
        
        # 重建张量
        reconstructed = reconstruct_from_cp(weights, factors)
        
        # 计算误差
        error = torch.norm(original_tensor - reconstructed) / torch.norm(original_tensor)
        errors.append(error.item())
        
        # 计算压缩率
        compressed_params = weights.numel() + sum(f.numel() for f in factors)
        compression_ratio = original_params / compressed_params
        compression_ratios.append(compression_ratio)
        
        # 可视化重建效果（显示第一个切片）
        axes[0, i].imshow(reconstructed[0].detach().numpy(), cmap='viridis')
        axes[0, i].set_title(f'Rank-{rank} Reconstruction')
        axes[0, i].axis('off')
        
        # 显示误差
        error_slice = torch.abs(original_tensor[0] - reconstructed[0])
        axes[1, i].imshow(error_slice.detach().numpy(), cmap='hot')
        axes[1, i].set_title(f'Error (Rank-{rank})')
        axes[1, i].axis('off')
    
    plt.tight_layout()
    plt.show()
    
    # 显示压缩率和误差统计
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
    
    ax1.plot(rank_list, compression_ratios, 'o-', markersize=8)
    ax1.set_title('Compression Ratio vs CP Rank')
    ax1.set_xlabel('CP Rank')
    ax1.set_ylabel('Compression Ratio')
    ax1.grid(True)
    
    ax2.plot(rank_list, errors, 'o-', markersize=8, color='red')
    ax2.set_title('Relative Error vs CP Rank')
    ax2.set_xlabel('CP Rank')
    ax2.set_ylabel('Relative Error')
    ax2.grid(True)
    
    plt.tight_layout()
    plt.show()
    
    return errors, compression_ratios

# 设置CP分解的秩
cp_rank = 4

# 进行CP分解
weights, factor_matrices = cp_decomposition(original_tensor, cp_rank)

# 可视化因子矩阵和权重
visualize_cp_factors(factor_matrices, weights)

# 重建张量
reconstructed_cp = reconstruct_from_cp(weights, factor_matrices)

# 可视化重建效果
visualize_3d_tensor(reconstructed_cp, 'CP Reconstructed Tensor')

# 计算CP分解的压缩效果
cp_original_params = original_tensor.numel()
cp_compressed_params = weights.numel() + sum(f.numel() for f in factor_matrices)
cp_compression_ratio = cp_original_params / cp_compressed_params
cp_relative_error = torch.norm(original_tensor - reconstructed_cp) / torch.norm(original_tensor)

print(f'CP分解 - 原始参数量: {cp_original_params}')
print(f'CP分解 - 压缩后参数量: {cp_compressed_params}')
print(f'CP分解 - 压缩率: {cp_compression_ratio:.2f}')
print(f'CP分解 - 相对误差: {cp_relative_error.item():.6f}')
```
    
![CP分解因子矩阵和权重可视化示例图](images/1.methods_of_low-rank_decomposition_22_0.png)
    
![重建张量可视化示例图](images/1.methods_of_low-rank_decomposition_22_1.png)

```
    CP分解 - 原始参数量: 240
    CP分解 - 压缩后参数量: 80
    CP分解 - 压缩率: 3.00
    CP分解 - 相对误差: 0.655080
```

我们还可以比较不同CP秩下的分解效果。
```python
cp_ranks = [2, 3, 4, 5]
cp_errors, cp_compression_ratios = compare_cp_ranks(original_tensor, cp_ranks)

# 打印详细结果
print("\nCP分解不同秩的详细结果:")
print("Rank\tCompression Ratio\tRelative Error")
print("----\t-----------------\t--------------")
for rank, ratio, error in zip(cp_ranks, cp_compression_ratios, cp_errors):
    print(f"{rank}\t{ratio:.2f}\t\t\t{error:.6f}")
```

![不同CP秩下的分解效果](images/1.methods_of_low-rank_decomposition_23_0.png)
  
![不同CP秩与压缩率和相对误差的关系](images/1.methods_of_low-rank_decomposition_23_1.png)
  
```    
    CP分解不同秩的详细结果:
    Rank	Compression Ratio	Relative Error
    ----	-----------------	--------------
    2	6.00			0.783377
    3	4.00			0.725117
    4	3.00			0.655080
    5	2.40			0.599979
```

上图展示了CP分解的效果。CP分解将张量分解为多个秩1张量的和，每个成分由三个因子向量的外积组成。

与Tucker分解相比，CP分解的结构更简单，没有核心张量，但在某些情况下可能需要更多的成分来达到相同的精度。CP分解特别适合处理具有多线性结构的数据。


**非负矩阵分解(NMF)。** 非负矩阵分解是一种约束矩阵分解方法，要求原始矩阵和分解得到的因子矩阵都是非负的。NMF将非负矩阵V分解为两个非负矩阵W和H的乘积：V ≈ WH。

NMF在图像处理、文本挖掘和推荐系统等领域有广泛应用，因为它能够学习到数据的部分表示。


```python
from sklearn.decomposition import NMF

def nmf_decomposition(matrix, n_components):
    """对非负矩阵进行NMF分解"""
    # 确保矩阵是非负的
    matrix_np = torch.abs(matrix).detach().numpy()
    
    # 进行NMF分解
    nmf = NMF(n_components=n_components, random_state=42, max_iter=1000)
    W = nmf.fit_transform(matrix_np)
    H = nmf.components_
    
    # 转回PyTorch张量
    W_tensor = torch.tensor(W, dtype=torch.float32)
    H_tensor = torch.tensor(H, dtype=torch.float32)
    
    return W_tensor, H_tensor

def visualize_nmf_decomposition(original, W, H, title_prefix="NMF"):
    """可视化NMF分解结果"""
    # 重建矩阵
    reconstructed = W @ H
    
    fig, axes = plt.subplots(2, 3, figsize=(15, 8))
    
    # 原始矩阵
    im1 = axes[0, 0].imshow(original.detach().numpy(), cmap='viridis')
    axes[0, 0].set_title('Original Matrix')
    plt.colorbar(im1, ax=axes[0, 0])
    
    # W矩阵 (基矩阵)
    im2 = axes[0, 1].imshow(W.detach().numpy(), cmap='viridis')
    axes[0, 1].set_title(f'{title_prefix} Basis Matrix W')
    plt.colorbar(im2, ax=axes[0, 1])
    
    # H矩阵 (系数矩阵)
    im3 = axes[0, 2].imshow(H.detach().numpy(), cmap='viridis')
    axes[0, 2].set_title(f'{title_prefix} Coefficient Matrix H')
    plt.colorbar(im3, ax=axes[0, 2])
    
    # 重建矩阵
    im4 = axes[1, 0].imshow(reconstructed.detach().numpy(), cmap='viridis')
    axes[1, 0].set_title('Reconstructed Matrix')
    plt.colorbar(im4, ax=axes[1, 0])
    
    # 误差
    error = torch.abs(original - reconstructed)
    im5 = axes[1, 1].imshow(error.detach().numpy(), cmap='hot')
    axes[1, 1].set_title('Reconstruction Error')
    plt.colorbar(im5, ax=axes[1, 1])
    
    # 基向量可视化
    n_components = W.shape[1]
    colors = plt.cm.Set3(np.linspace(0, 1, n_components))
    for i in range(min(n_components, 5)):  # 最多显示5个基向量
        axes[1, 2].plot(W[:, i].detach().numpy(), color=colors[i], 
                       label=f'Basis {i+1}', linewidth=2)
    axes[1, 2].set_title('Basis Vectors')
    axes[1, 2].legend()
    axes[1, 2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

def compare_nmf_components(original_matrix, component_list):
    """比较不同成分数下的NMF效果"""
    errors = []
    compression_ratios = []
    original_params = original_matrix.numel()
    
    fig, axes = plt.subplots(2, len(component_list), figsize=(4*len(component_list), 8))
    
    for i, n_comp in enumerate(component_list):
        # NMF分解
        W, H = nmf_decomposition(original_matrix, n_comp)
        reconstructed = W @ H
        
        # 计算误差和压缩率
        error = torch.norm(original_matrix - reconstructed) / torch.norm(original_matrix)
        errors.append(error.item())
        
        compressed_params = W.numel() + H.numel()
        compression_ratio = original_params / compressed_params
        compression_ratios.append(compression_ratio)
        
        # 可视化
        axes[0, i].imshow(reconstructed.detach().numpy(), cmap='viridis')
        axes[0, i].set_title(f'Components={n_comp}')
        axes[0, i].axis('off')
        
        error_img = torch.abs(original_matrix - reconstructed)
        axes[1, i].imshow(error_img.detach().numpy(), cmap='hot')
        axes[1, i].set_title(f'Error (Comp={n_comp})')
        axes[1, i].axis('off')
    
    plt.tight_layout()
    plt.show()
    
    # 显示统计结果
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
    
    ax1.plot(component_list, compression_ratios, 'o-', markersize=8, color='blue')
    ax1.set_title('Compression Ratio vs Components')
    ax1.set_xlabel('Number of Components')
    ax1.set_ylabel('Compression Ratio')
    ax1.grid(True)
    
    ax2.plot(component_list, errors, 'o-', markersize=8, color='red')
    ax2.set_title('Relative Error vs Components')
    ax2.set_xlabel('Number of Components')
    ax2.set_ylabel('Relative Error')
    ax2.grid(True)
    
    plt.tight_layout()
    plt.show()
    
    return errors, compression_ratios

# 创建一个非负矩阵进行NMF分解
nmf_matrix = torch.abs(torch.randn(12, 10)) + 0.1  # 确保非负

print("非负矩阵分解(NMF)实验:")

# 设置成分数
n_components = 4

# 进行NMF分解
W, H = nmf_decomposition(nmf_matrix, n_components)

# 可视化分解结果
visualize_nmf_decomposition(nmf_matrix, W, H)

# 计算压缩效果
nmf_original_params = nmf_matrix.numel()
nmf_compressed_params = W.numel() + H.numel()
nmf_compression_ratio = nmf_original_params / nmf_compressed_params
nmf_reconstructed = W @ H
nmf_relative_error = torch.norm(nmf_matrix - nmf_reconstructed) / torch.norm(nmf_matrix)

print(f'NMF - 原始参数量: {nmf_original_params}')
print(f'NMF - 压缩后参数量: {nmf_compressed_params}')
print(f'NMF - 压缩率: {nmf_compression_ratio:.2f}')
print(f'NMF - 相对误差: {nmf_relative_error.item():.6f}')
```

    
![png](images/1.methods_of_low-rank_decomposition_27_1.png)
    
```
    NMF - 原始参数量: 120
    NMF - 压缩后参数量: 88
    NMF - 压缩率: 1.36
    NMF - 相对误差: 0.272304
```

我们还可以比较不同成分数的NMF效果。
```python
component_list = [2, 3, 4, 6]
nmf_errors, nmf_compression_ratios = compare_nmf_components(nmf_matrix, component_list)

print("\nNMF不同成分数的详细结果:")
print("Components\tCompression Ratio\tRelative Error")
print("----------\t-----------------\t--------------")
for comp, ratio, error in zip(component_list, nmf_compression_ratios, nmf_errors):
    print(f"{comp}\t\t{ratio:.2f}\t\t\t{error:.6f}")
```

![png](images/1.methods_of_low-rank_decomposition_28_0.png)
      
![png](images/1.methods_of_low-rank_decomposition_28_1.png)

```   
    NMF不同成分数的详细结果:
    Components	Compression Ratio	Relative Error
    ----------	-----------------	--------------
    2		2.73			0.413810
    3		1.82			0.343443
    4		1.36			0.272304
    6		0.91			0.174703
```

上图展示了非负矩阵分解的效果。NMF将原始矩阵分解为基矩阵W和系数矩阵H，其中W的每一列代表一个基向量，H的每一行代表对应基向量的系数。

NMF的优势在于其非负约束使得分解结果具有很好的可解释性，基向量通常对应数据的某些局部特征或模式。右下角的基向量图显示了学习到的不同模式。

与SVD等方法相比，NMF更适合处理本质上非负的数据，如图像像素值、文档词频等。

我们展示了四种重要的低秩分解方法：

- **SVD分解**：通用的矩阵分解方法，适用于任意矩阵，在图像压缩等领域应用广泛。
- **Tucker分解**：高阶张量分解，通过核心张量和因子矩阵实现灵活的多维数据压缩。
- **CP分解**：将张量表示为秩1张量的和，结构简单，适合多线性数据。
- **NMF分解**：约束分解方法，保证非负性，结果具有良好的可解释性。

每种方法都有其特定的应用场景和优势，在神经网络压缩中可以根据数据特性选择合适的分解方法。

### 模型低秩分解压缩实战

本实践将利用低秩分解方法中的SVD（奇异值分解）对预训练的LeNet模型进行低秩分解，以达到模型压缩的目的。完整代码见[模型低秩分解压缩实战](https://github.com/datawhalechina/awesome-compression/blob/main/docs/notebook/ch07/2.model_compression_practice.ipynb)。
我们将比较不同压缩率下的模型精度，并探索两种压缩策略：
1.  同时压缩卷积层和全连接层
2.  仅压缩全连接层

实验中，"压缩率"指的是参数减少的比例。例如，压缩率为0.3意味着模型参数量减少30%，即新模型的参数量约为原模型的70%。

首先，我们定义LeNet模型结构、数据加载函数和模型评估函数。

```python
import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import numpy as np
import matplotlib.pyplot as plt
import copy
import torch.nn.functional as F

# 设置matplotlib字体以支持中文
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

# 定义LeNet模型
class LeNet(nn.Module):
    def __init__(self, num_classes=10):
        super(LeNet, self).__init__()
        self.conv1 = nn.Conv2d(1, 6, 5)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.maxpool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(16 * 4 * 4, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, num_classes)

    def forward(self, x):
        x = self.maxpool(F.relu(self.conv1(x)))
        x = self.maxpool(F.relu(self.conv2(x)))
        
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# 设置设备
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 加载MNIST数据集
def get_mnist_loaders(batch_size=256):
    """获取MNIST测试数据加载器"""
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    test_dataset = torchvision.datasets.MNIST(root='../ch02/data/mnist', train=False, download=True, transform=transform)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    return test_loader

# 评估模型准确率
@torch.no_grad()
def evaluate_model(model, test_loader):
    """在测试集上评估模型准确率"""
    model.eval()
    correct = 0
    total = 0
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
    accuracy = 100 * correct / total
    return accuracy

# 加载原始模型
original_model = LeNet().to(device)
try:
    # 确保模型路径正确
    original_model.load_state_dict(torch.load('../ch02/model.pt', map_location=device))
except FileNotFoundError:
    print("错误：找不到预训练模型 '../ch02/model.pt'。请先运行ch02的训练脚本。")
    # 如果找不到预训练模型，则使用随机初始化的模型，准确率会很低
    original_model = LeNet().to(device)

# 获取测试数据
test_loader = get_mnist_loaders()

# 评估原始模型
baseline_accuracy = evaluate_model(original_model, test_loader)
print(f'原始模型准确率: {baseline_accuracy:.2f}%')
```


接下来，我们定义执行SVD分解和压缩的核心函数。我们将分别为全连接层和卷积层创建压缩函数。


```python
def count_parameters(model):
    """计算模型的总参数量"""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)
def svd_compress_linear(layer, compression_rate):
    """使用SVD压缩全连接层"""
    # 提取权重和偏置
    weight = layer.weight.data
    bias = layer.bias.data
    
    # 计算原始参数量
    in_features = layer.in_features
    out_features = layer.out_features
    original_params = in_features * out_features
    
    # 根据压缩率计算目标秩
    # 新参数量 = (in_features * rank) + (rank * out_features)
    # (1 - compression_rate) * original_params = rank * (in_features + out_features)
    target_rank = int((1 - compression_rate) * original_params / (in_features + out_features))
    
    # 保证秩至少为1且不超过最大可能秩
    max_rank = min(in_features, out_features)
    rank = max(1, min(target_rank, max_rank))
    
    # SVD分解
    U, S, V = torch.svd(weight)
    
    # 构建两个新的权重矩阵
    # W ≈ U_k @ S_k @ V_k.T
    # W' = A @ B, 其中 A = U_k @ sqrt(S_k), B = sqrt(S_k) @ V_k.T
    A = U[:, :rank] @ torch.diag(torch.sqrt(S[:rank]))
    B = torch.diag(torch.sqrt(S[:rank])) @ V[:, :rank].T
    
    # 创建新的两个线性层
    layer1 = nn.Linear(in_features, rank, bias=False).to(device)
    layer2 = nn.Linear(rank, out_features, bias=True).to(device)
    
    # 加载新的权重和原始偏置
    layer1.weight.data = B
    layer2.weight.data = A
    layer2.bias.data = bias
    
    # 返回一个包含两个新层的nn.Sequential模块
    return nn.Sequential(layer1, layer2)

def svd_compress_conv(layer, compression_rate):
    """使用SVD压缩卷积层"""
    # 提取权重和偏置
    weight = layer.weight.data
    bias = layer.bias.data
    
    # 获取卷积层参数
    out_channels, in_channels, kernel_size, _ = weight.shape
    stride = layer.stride
    padding = layer.padding
    
    # 将权重张量重塑为矩阵 (out_channels, in_channels * k * k)
    weight_matrix = weight.view(out_channels, -1)
    
    # 计算原始参数量和新参数量的单位
    original_params = weight_matrix.numel()
    params_per_rank = (in_channels * kernel_size * kernel_size) + out_channels
    
    # 根据压缩率计算目标秩
    target_rank = int((1 - compression_rate) * original_params / params_per_rank)
    rank = max(1, min(target_rank, out_channels))
    
    # SVD分解
    U, S, V = torch.svd(weight_matrix)
    
    # 构建两个新的权重矩阵
    # A(out_channels, rank) and B(rank, in_channels*k*k)
    A = U[:, :rank]
    B = torch.diag(S[:rank]) @ V[:, :rank].T
    
    # 创建新的两个卷积层
    # 第一个卷积层保持原始卷积核大小
    conv1 = nn.Conv2d(in_channels, rank, kernel_size=kernel_size, 
                      stride=stride, padding=padding, bias=False).to(device)
    # 第二个是1x1卷积
    conv2 = nn.Conv2d(rank, out_channels, kernel_size=1, 
                      stride=1, padding=0, bias=True).to(device)
    
    # 加载新的权重和原始偏置
    conv1.weight.data = B.view(rank, in_channels, kernel_size, kernel_size)
    conv2.weight.data = A.view(out_channels, rank, 1, 1)
    conv2.bias.data = bias
    
    return nn.Sequential(conv1, conv2)
```

> 实验一：压缩卷积层与全连接层

在此实验中，我们将对LeNet模型的所有卷积层（`conv1`, `conv2`）和全连接层（`fc1`, `fc2`, `fc3`）应用SVD压缩，并观察不同压缩率对模型精度的影响。

```python
compression_rates = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]
accuracies_exp1 = []
original_params = count_parameters(original_model)

print("实验一：压缩所有层")
print(f"原始模型总参数量: {original_params}")
for rate in compression_rates:
    # 深度拷贝原始模型，避免修改
    compressed_model = copy.deepcopy(original_model)
    
    # 压缩卷积层
    compressed_model.conv1 = svd_compress_conv(compressed_model.conv1, rate)
    compressed_model.conv2 = svd_compress_conv(compressed_model.conv2, rate)
    
    # 压缩全连接层
    compressed_model.fc1 = svd_compress_linear(compressed_model.fc1, rate)
    compressed_model.fc2 = svd_compress_linear(compressed_model.fc2, rate)
    compressed_model.fc3 = svd_compress_linear(compressed_model.fc3, rate)
    
    # 评估压缩后的模型
    compressed_params = count_parameters(compressed_model)
    accuracy = evaluate_model(compressed_model, test_loader)
    accuracies_exp1.append(accuracy)
    
    
    print(f'压缩率: {rate*100:.0f}% -> 压缩后参数量: {compressed_params} -> 准确率: {accuracy:.2f}%')
```
输出结果如下：
```
    实验一：压缩所有层
    原始模型总参数量: 44426
    压缩率: 20% -> 压缩后参数量: 35209 -> 准确率: 95.80%
    压缩率: 30% -> 压缩后参数量: 30921 -> 准确率: 92.75%
    压缩率: 40% -> 压缩后参数量: 26436 -> 准确率: 79.09%
    压缩率: 50% -> 压缩后参数量: 21772 -> 准确率: 63.92%
    压缩率: 60% -> 压缩后参数量: 17287 -> 准确率: 38.61%
    压缩率: 70% -> 压缩后参数量: 12999 -> 准确率: 23.00%
```

从上面的结果可以看出，随着压缩率的提高，模型的准确率逐渐下降。在压缩率较低时（如20%-30%），准确率下降不明显，说明模型中存在较多冗余参数。当压缩率进一步提高时，准确率开始显著下降，因为更多的关键信息被丢弃。



> 实验二：仅压缩全连接层

在此实验中，我们只对全连接层（`fc1`, `fc2`, `fc3`）进行SVD压缩，保持卷积层不变。这有助于我们判断模型参数的冗余主要存在于哪部分。


```python
accuracies_exp2 = []

print("\n实验二：仅压缩全连接层")
print(f"原始模型总参数量: {original_params}")
for rate in compression_rates:
    compressed_model = copy.deepcopy(original_model)
    
    # 仅压缩全连接层
    compressed_model.fc1 = svd_compress_linear(compressed_model.fc1, rate)
    compressed_model.fc2 = svd_compress_linear(compressed_model.fc2, rate)
    compressed_model.fc3 = svd_compress_linear(compressed_model.fc3, rate)

    # 评估压缩后的模型
    compressed_params = count_parameters(compressed_model)
    
    accuracy = evaluate_model(compressed_model, test_loader)
    accuracies_exp2.append(accuracy)
    
    
    print(f'压缩率: {rate*100:.0f}% -> 压缩后参数量: {compressed_params} -> 准确率: {accuracy:.2f}%')
```

输出结果如下：
```
    实验二：仅压缩全连接层
    原始模型总参数量: 44426
    压缩率: 20% -> 压缩后参数量: 35840 -> 准确率: 96.76%
    压缩率: 30% -> 压缩后参数量: 31718 -> 准确率: 95.02%
    压缩率: 40% -> 压缩后参数量: 27596 -> 准确率: 89.79%
    压缩率: 50% -> 压缩后参数量: 23098 -> 准确率: 75.52%
    压缩率: 60% -> 压缩后参数量: 18976 -> 准确率: 61.63%
    压缩率: 70% -> 压缩后参数量: 14854 -> 准确率: 46.57%
```

与实验一相比，仅压缩全连接层时，模型准确率的下降速度更慢。这表明在LeNet模型中，全连接层的参数冗余度更高，更容易被压缩。而卷积层对模型性能的影响更为关键，压缩卷积层会导致更快的精度损失。

最后，我们将两个实验的结果绘制在同一张图表中，以便更直观地进行比较。

```python
plt.figure(figsize=(10, 6))
plt.plot([r * 100 for r in compression_rates], accuracies_exp1, 'o-', label='Experiment 1: Compress All Layers')
plt.plot([r * 100 for r in compression_rates], accuracies_exp2, 's-', label='Experiment 2: Compress FC Layers Only')
plt.axhline(y=baseline_accuracy, color='r', linestyle='--', label=f'Baseline Accuracy ({baseline_accuracy:.2f}%)')

plt.title('Comparison of model accuracy under different compression strategies')
plt.xlabel('Compression rate (%)')
plt.ylabel('Accuracy (%)')
plt.legend()
plt.grid(True)
plt.show()
```

![实验结果对比](images/2.model_compression_practice_16_1.png)
    
通过以上两个实验和可视化对比，我们可以得出以下结论：

- **低秩分解是有效的模型压缩方法**：通过低秩近似，我们可以在一定程度上减少模型参数，而不会导致灾难性的精度下降。
- **全连接层冗余度更高**：在LeNet这类模型中，全连接层通常占据大量参数，且冗余度较高。仅压缩全连接层可以在保持较高精度的同时实现可观的压缩效果。
- **压缩策略的重要性**：选择合适的压缩目标（哪些层）和压缩率是模型压缩的关键。盲目地对所有层应用高压缩率可能会严重损害模型性能。

## 引用资料

- [MIT 6.5940 TinyML and Efficient Deep Learning Computing](https://hanlab.mit.edu/courses/2023-fall-65940)
